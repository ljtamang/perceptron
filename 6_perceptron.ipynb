{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, weights):\n",
    "    \"\"\" Predict the output label\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        :x: {array-like}, shape = [n_features]\n",
    "                  Single Training record where n_features \n",
    "                  is the number of features in the training record.\n",
    "        :weights: array-like, shape = [n_features+1]\n",
    "                  Weights to be learned\n",
    "        :return:  output : float \n",
    "                  Predicted traget label of the traing \n",
    "\n",
    "        \"\"\"\n",
    "    bias = weights[0] # First weight is bias term\n",
    "    net_input = np.matmul(x, weights[1:]) + bias\n",
    "    output = np.where(net_input>0.0, 1, -1)\n",
    "    return output\n",
    "\n",
    "def fit(x_train, y_train,learning_rate=0.1, no_of_epochs=500):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        :x_train: {array-like}, shape = [n_samples, n_features]\n",
    "                  Training recoreds, where n_samples\n",
    "                  is the number of records and\n",
    "                  n_features is the number of features of each record.\n",
    "        :y_train: array-like, shape = [n_targets]\n",
    "        :learning_rate: float, \n",
    "                  learning rate of the perceptron\n",
    "        :no_of_epochs: float, \n",
    "                  no of epochs to learn the weight\n",
    "        :return:  weight : array-like, shape = [n_weights]\n",
    "                  weight learnded in each eopchs\n",
    "        :return:  errors_percent : array-like, shape = [n_errors]\n",
    "                  errors of each epochs\n",
    "\n",
    "        \"\"\"\n",
    "        weights = np.random.rand(1+x_train.shape[1])\n",
    "        errors_percent = [] # Stores error % for each epochs in percentage\n",
    "        for epochs in range(no_of_epochs):\n",
    "            errors = 0\n",
    "            for x,target in zip(x_train, y_train): \n",
    "                output = predict(x, weights) # find predicted output\n",
    "                weights_to_update = learning_rate*(target-output)\n",
    "                weights[1:] += weights_to_update*x # update the weights excepts bias\n",
    "                weights[0] += weights_to_update # update bias\n",
    "                errors += int(weights_to_update != 0.0) # add 1 if output and target are not same\n",
    "            erro_in_percent = float(100*float(errors)/float(len(x_train)))\n",
    "            errors_percent.append(erro_in_percent) # add error percentage in each epochs\n",
    "            \n",
    "        return weights, errors_percent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset1(csvFilename):\n",
    "    \"\"\" Load data from csv.\n",
    "\n",
    "        Parameter\n",
    "        ----------\n",
    "        :csvFilename: String\n",
    "                      Name of the csv file which contains data\n",
    "        :return     : dataset: {array-like}, shape = [n_rows, n_columns]\n",
    "                      Records where n_rows is the number of records\n",
    "                      and n_columns is the features in each record.\n",
    "                \n",
    "        \"\"\"\n",
    "    dataset = genfromtxt(csvFilename, delimiter=',', skip_header=1)\n",
    "    no_columns = dataset.shape[1]\n",
    "    x_train = dataset[:,:no_columns-1]\n",
    "    y_train = dataset[:,no_columns-1]\n",
    "    return x_train, y_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset2(csvFilename):\n",
    "    \"\"\" Load data from csv.\n",
    "\n",
    "        Parameter\n",
    "        ----------\n",
    "        :csvFilename: String\n",
    "                      Name of the csv file which contains data\n",
    "        :return     : dataset: {array-like}, shape = [n_rows, n_columns]\n",
    "                      Records where n_rows is the number of records\n",
    "                      and n_columns is the features in each record.\n",
    "                \n",
    "        \"\"\"\n",
    "    # find number of columns\n",
    "    with open(csvFilename) as f:\n",
    "        ncols = len(f.readline().split(','))\n",
    "    # Read training data\n",
    "    x_train = genfromtxt(csvFilename, delimiter=',',skip_header=1,usecols=range(0,ncols-1))\n",
    "    # Read target data\n",
    "    #for d-100 to convert NS->-1 and S-> 1\n",
    "    convertfunc = lambda x: -1 if x==b'NS' else 1\n",
    "    y_train=genfromtxt(csvFilename, delimiter=',',skip_header=1,usecols=range(ncols-1,ncols), converters={ncols-1: convertfunc})\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(dataset, learning_rate, errors_percent):\n",
    "    \"\"\" Display the result.\n",
    "    \"\"\"\n",
    "    print(\"Dataset name: %s\" % (dataset))\n",
    "    print(\"learning rate: %.3f\" % (learning_rate))\n",
    "    print(\"Error at 100 epochs: %.3f\" % (errors_percent[99]))\n",
    "    print(\"Error at 500 epochs %.3f\" % (errors_percent[499]))\n",
    "    print(\"*******************************\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: d-10.csv\n",
      "learning rate: 0.010\n",
      "Error at 100 epochs: 1.200\n",
      "Error at 500 epochs 3.000\n",
      "*******************************\n",
      "Dataset name: d-10.csv\n",
      "learning rate: 0.100\n",
      "Error at 100 epochs: 4.000\n",
      "Error at 500 epochs 1.000\n",
      "*******************************\n",
      "Dataset name: d-10.csv\n",
      "learning rate: 0.200\n",
      "Error at 100 epochs: 3.800\n",
      "Error at 500 epochs 0.000\n",
      "*******************************\n"
     ]
    }
   ],
   "source": [
    "# Train with d-10.csv  data\n",
    "dataset = \"d-10.csv\"\n",
    "no_of_epochs = 500\n",
    "learning_rate = 0.01\n",
    "x_train, y_train  = load_dataset1(dataset) # load data\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) # fit the data to learn the weights\n",
    "display(dataset, learning_rate, errors_percent)\n",
    "\n",
    "learning_rate = 0.1\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) \n",
    "display(dataset, learning_rate, errors_percent)\n",
    "\n",
    "learning_rate = 0.2\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) \n",
    "display(dataset, learning_rate, errors_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: d-100.csv\n",
      "learning rate: 0.010\n",
      "Error at 100 epochs: 3.222\n",
      "Error at 500 epochs 2.024\n",
      "*******************************\n",
      "Dataset name: d-100.csv\n",
      "learning rate: 0.100\n",
      "Error at 100 epochs: 3.208\n",
      "Error at 500 epochs 1.836\n",
      "*******************************\n",
      "Dataset name: d-100.csv\n",
      "learning rate: 0.200\n",
      "Error at 100 epochs: 3.250\n",
      "Error at 500 epochs 1.812\n",
      "*******************************\n"
     ]
    }
   ],
   "source": [
    "#  training with \"d-100.csv\"\n",
    "dataset2 = \"d-100.csv\"\n",
    "no_of_epochs = 500\n",
    "learning_rate = 0.01\n",
    "x_train, y_train  = load_dataset2(dataset2) # load data\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) # fit the data to learn the weights\n",
    "display(dataset2, learning_rate, errors_percent)\n",
    "\n",
    "learning_rate = 0.1\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) \n",
    "display(dataset2, learning_rate, errors_percent)\n",
    "\n",
    "learning_rate = 0.2\n",
    "_, errors_percent = fit(x_train, y_train , learning_rate, no_of_epochs) \n",
    "display(dataset2, learning_rate, errors_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
